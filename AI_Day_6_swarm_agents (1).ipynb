{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
        "outputId": "9b22a4a8-8577-4734-9d09-1d924a20c223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/swarm.git\n",
            "  Cloning https://github.com/openai/swarm.git to /tmp/pip-req-build-we93a6jb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/swarm.git /tmp/pip-req-build-we93a6jb\n",
            "  Resolved https://github.com/openai/swarm.git to commit 9db581cecaacea0d46a933d6453c312b034dbf47\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.33.0 in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.54.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (8.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (4.66.6)\n",
            "Collecting pre-commit (from swarm==0.1.0)\n",
            "  Downloading pre_commit-4.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting instructor (from swarm==0.1.0)\n",
            "  Downloading instructor-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.11.2)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.1.4)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.33.0->swarm==0.1.0)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (2.23.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2024.8.30)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading identify-2.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->swarm==0.1.0) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading virtualenv-20.28.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->swarm==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (1.5.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (0.1.2)\n",
            "Downloading instructor-1.7.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.0.1-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading identify-2.6.3-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: swarm\n",
            "  Building wheel for swarm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swarm: filename=swarm-0.1.0-py3-none-any.whl size=25999 sha256=965de2633a9d3822de68556947bc724f5cb28ffff2b9e42ca8f6f0df38486d32\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ad__imwv/wheels/46/9a/f7/7b8bbb674ae80ef0f62a632706c2c4cdfcf708e4da32e4e256\n",
            "Successfully built swarm\n",
            "Installing collected packages: distlib, virtualenv, nodeenv, jiter, identify, cfgv, pre-commit, instructor, swarm\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.7.1\n",
            "    Uninstalling jiter-0.7.1:\n",
            "      Successfully uninstalled jiter-0.7.1\n",
            "Successfully installed cfgv-3.4.0 distlib-0.3.9 identify-2.6.3 instructor-1.7.0 jiter-0.6.1 nodeenv-1.9.1 pre-commit-4.0.1 swarm-0.1.0 virtualenv-20.28.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting firecrawl-py\n",
            "  Downloading firecrawl_py-1.6.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl-py)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting websockets (from firecrawl-py)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2024.8.30)\n",
            "Downloading firecrawl_py-1.6.2-py3-none-any.whl (16 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, python-dotenv, firecrawl-py\n",
            "Successfully installed firecrawl-py-1.6.2 python-dotenv-1.0.1 websockets-14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/swarm.git\n",
        "!pip install openai\n",
        "!pip install firecrawl-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
        "outputId": "393a76c4-58a8-44c8-f2c2-b93fdbc9db5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"schema\" in \"FirecrawlApp.ExtractParams\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "from swarm import Agent, Swarm\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from openai import ChatCompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
        "outputId": "9931d84e-6de4-4143-ce69-1d549986295f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Web Analytics Results:\n",
            "Here's the statistical summary and insights from your dataset:\n",
            "\n",
            "### Overview of the Dataset\n",
            "\n",
            "- **Total Entries**: 52,721\n",
            "\n",
            "#### Key Fields:\n",
            "\n",
            "1. **Date**\n",
            "   - Range: January 1, 2020 - September 30, 2020\n",
            "   - Mean: May 19, 2020\n",
            "\n",
            "2. **Source**\n",
            "   - Unique Sources: 22\n",
            "   - Most Frequent Source: Facebook (12,954 occurrences)\n",
            "\n",
            "3. **Medium**\n",
            "   - Unique Mediums: 6\n",
            "   - Most Frequent Medium: CPC (16,833 occurrences)\n",
            "\n",
            "4. **Delivery Available**\n",
            "   - Unique Categories: 3\n",
            "   - Most Common Category: No data (31,953 occurrences)\n",
            "\n",
            "5. **Device Type**\n",
            "   - Unique Device Types: 3\n",
            "   - Most Common Device Type: Mobile (24,416 occurrences)\n",
            "\n",
            "6. **Promo Activated**\n",
            "   - Unique Categories: 2\n",
            "   - Most Common Category: No (27,548 occurrences)\n",
            "\n",
            "7. **Filter Used**\n",
            "   - Unique Categories: 2\n",
            "   - Most Common Category: No (37,290 occurrences)\n",
            "\n",
            "#### Engagement Metrics:\n",
            "- **Pageviews**\n",
            "  - Mean: 583.76\n",
            "  - Min: 0\n",
            "  - Max: 34,832\n",
            "  - Outliers indicated by a high standard deviation (1452.00) which reveals variability in pageviews across entries.\n",
            "\n",
            "- **Visits**\n",
            "  - Mean: 127.12\n",
            "  - Min: 1\n",
            "  - Max: 6,975\n",
            "  - Standard Deviation: 384.87\n",
            "\n",
            "- **Product Clicks**\n",
            "  - Mean: 890.76\n",
            "  - Min: 0\n",
            "  - Max: 32,460\n",
            "  - Standard Deviation: 2100.56\n",
            "\n",
            "- **Add to Cart**\n",
            "  - Mean: 177.97\n",
            "  - Min: 0\n",
            "  - Max: 6,486\n",
            "  - Standard Deviation: 419.90\n",
            "\n",
            "- **Checkout**\n",
            "  - Mean: 187.59\n",
            "  - Min: 0\n",
            "  - Max: 4,936.96\n",
            "  - Standard Deviation: 441.13\n",
            "\n",
            "- **Transactions**\n",
            "  - Mean: 20.09\n",
            "  - Min: 0\n",
            "  - Max: 1,113\n",
            "  - Standard Deviation: 52.43\n",
            "\n",
            "- **Revenue**\n",
            "  - Unique Revenue values: 21,242 with ₱0 being the most frequent (14,292 occurrences)\n",
            "\n",
            "- **Ad Spend**\n",
            "  - Unique Ad Spend values: 51,201, with ₱869,459 being the most frequent (3 occurrences)\n",
            "\n",
            "### Null Values\n",
            "- No null values found in any fields.\n",
            "\n",
            "### Correlations (selected examples):\n",
            "- **Pageviews and Visits**: Strong correlation (0.84)\n",
            "- **Pageviews and Product Clicks**: Strong correlation (0.86)\n",
            "- **Add to Cart and Product Clicks**: Near perfect correlation (0.9999)\n",
            "\n",
            "### Conclusion\n",
            "The dataset shows a wide variety of user engagement metrics, with significant variations particularly in pageviews and product clicks. The correlations suggest strong relationships between user activities, which can be useful for further analysis or strategic planning. \n",
            "\n",
            "If you need further analysis or visualizations, feel free to ask!\n",
            "\n",
            "Web Insights Results:\n",
            "Based on the dataset provided, here are the key insights regarding your web traffic, including source pageviews, visits, transactions, and product clicks:\n",
            "\n",
            "### Summary of Insights\n",
            "\n",
            "1. **Top Traffic Sources**:\n",
            "   - **Google** and **Facebook** are the leading traffic sources for your website, with Google generating **12,296,272 pageviews** and **2,497,758 visits**, while Facebook has **11,915,408 pageviews** and **2,464,741 visits**.\n",
            "   - Other notable sources include **Tiktok** with **2,072,820 pageviews** and **475,415 visits**, and **Instagram** with **426,249 pageviews** and **107,464 visits**.\n",
            "\n",
            "2. **Engagement Metrics**:\n",
            "   - Although Google and Facebook are top sources for traffic, **Facebook** outperforms in transaction metrics, yielding **377,909 transactions**, while Google follows closely with **388,169 transactions**.\n",
            "   - The engagement shows that while Google and Facebook drive traffic, Facebook converts visits into transactions better.\n",
            "\n",
            "3. **Conversion Rates**:\n",
            "   - The completion of transactions from visits is highest from Google (15.5%) and Facebook (15.3%).\n",
            "   - **Direct traffic** shows a significant number of pageviews (2,929,713) and visits (883,431), with a conversion reflecting a promising status at **22.3%** of visits leading to transactions.\n",
            "\n",
            "4. **Lower-Performing Sources**:\n",
            "   - Sources such as **Baidu** and **YouTube** lag behind, with Baidu reporting **0 transactions** and YouTube **0 transactions** despite some pageviews.\n",
            "   - This highlights a potential area for improvement, specifically in optimizing these platforms for better conversion rates.\n",
            "\n",
            "5. **Promotional Campaign Effectiveness**:\n",
            "   - **Promo campaigns** generated **130,727 pageviews** and **32,851 visits**, leading to **4,395 transactions**. This reflects a moderate conversion that could benefit from enhanced targeting or promotion strategies.\n",
            "\n",
            "### Actionable Recommendations\n",
            "\n",
            "1. **Enhance Facebook Strategy**: Since Facebook shows higher transaction rates, consider investing more in targeted ads and promotional content on this platform to drive user engagement and conversion.\n",
            "\n",
            "2. **Optimize Google Campaigns**: Improve your SEO and PPC strategies on Google, which remains a significant source of traffic and transactions.\n",
            "\n",
            "3. **Bolster Direct Traffic**: Given the high performance of direct traffic, focus on building your brand and customer loyalty programs to encourage repeat visits.\n",
            "\n",
            "4. **Focus on Underperforming Channels**: Investigate ways to optimize Baidu and YouTube channels. Experiment with tailored content and advertising strategies to boost engagement and conversions.\n",
            "\n",
            "5. **Evaluate Promotional Initiatives**: Assess the current promotional initiatives to identify which campaigns yield the best conversion rates and allocate budget resources accordingly to maximize returns.\n",
            "\n",
            "6. **Further Analysis**: Conduct a deeper analysis into traffic quality from various sources to ensure high engagement and conversion instead of just focusing on volume.\n",
            "\n",
            "By implementing these recommendations, you can enhance overall website performance, foster user engagement, and increase conversion rates.\n",
            "\n",
            "Web Analytics Visuals Results:\n",
            "I can guide you on how to create visualizations using Seaborn or Matplotlib, but I would need to know the structure of the dataset you're referring to. Could you provide details about your dataset, such as the columns and types of data it includes? If you have specific visualizations in mind (e.g., bar charts, scatter plots, histograms), please let me know that as well!\n"
          ]
        }
      ],
      "source": [
        "# Configure API Keys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]= \"\"\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jaydiaz2012/AI_First_Day_6_AI_Swarm/refs/heads/main/ai%20first%20sales%20data%20-%20sales%20(1).csv\")\n",
        "\n",
        "# Analyze dataset\n",
        "def analyze_dataset():\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    numerical_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "    summary = {\n",
        "        \"overview\": df.describe(include=\"all\").to_dict(),\n",
        "        \"null_values\": df.isnull().sum().to_dict(),\n",
        "        \"correlations\": numerical_df.corr().to_dict(),\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "def generate_web_insights():\n",
        "    web_insights = {\n",
        "        'source_pageviews': df.groupby('source')['pageviews'].sum(),\n",
        "        'source_visits': df.groupby('source')['visits'].sum(),\n",
        "        'source_transactions': df.groupby('source')['transactions'].sum(),\n",
        "        'source_product_click': df.groupby('source')['productClick'].sum()\n",
        "    }\n",
        "    return web_insights\n",
        "\n",
        "def generate_web_visuals():\n",
        "    visualizations = {}\n",
        "\n",
        "    # Page Views Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"pageviews\")\n",
        "    plt.title(\"Page Views Distribution\")\n",
        "    visualizations['page_views_distribution']\n",
        "    plt.show()\n",
        "\n",
        "    # Visits Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"visits\")\n",
        "    plt.title(\"Visits Distribution\")\n",
        "    visualizations['visits_distribution']\n",
        "    plt.show()\n",
        "\n",
        "    # Transactions Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"transactions\")\n",
        "    plt.title(\"Transactions Trends\")\n",
        "    visualizations['transactions_trends']\n",
        "    plt.show()\n",
        "\n",
        "    # Product Click Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"productClick\")\n",
        "    plt.title(\"Product Clicks Trends\")\n",
        "    visualizations['product_click_trends']\n",
        "    plt.show()\n",
        "\n",
        "    return visualizations\n",
        "\n",
        "web_analytics_agent = Agent(\n",
        "    name=\"Web Analyst Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a web analyst agent that cleans the dataset and report on the statistical summary of the dataset\"\"\",\n",
        "    functions=[analyze_dataset],\n",
        ")\n",
        "\n",
        "web_insights_agent = Agent(\n",
        "    name=\"Web Insights Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a Web Insights Agent that provide insights about the data provided and summarize the insights from other agents, including actionable recommendations\"\"\",\n",
        "    functions=[generate_web_insights]\n",
        ")\n",
        "\n",
        "web_insights_visuals_agent = Agent(\n",
        "    name=\"Web_Visuals_Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a data visualization expert that create visualizations on a given datase and gives summary of the data visuals\"\"\",\n",
        "    fuctions=[generate_web_visuals]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    client = Swarm()\n",
        "\n",
        "    web_analytics_response = client.run(\n",
        "        agent=web_analytics_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please clean and analyze my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Results:\")\n",
        "    print(web_analytics_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_response = client.run(\n",
        "        agent=web_insights_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide insights based from my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Insights Results:\")\n",
        "    print(web_insights_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_visuals_response = client.run(\n",
        "        agent=web_insights_visuals_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide charts using Seaborn or Matplotlib using the data from the given dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Visuals Results:\")\n",
        "    print(web_insights_visuals_response.messages[-1][\"content\"])\n",
        "\n",
        "\n",
        "# Advanced analysis using Swarm\n",
        "def advanced_analysis(prompt, model):\n",
        "    try:\n",
        "        prompt = (\n",
        "            \"Perform clustering, detect anomalies, and provide predictive trends \"\n",
        "            f\"on this dataset:\\n{df.to_csv(index=False)}\"\n",
        "        )\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a data analysis assistant using Swarm.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error performing advanced analysis: {e}\")\n",
        "        return None\n",
        "    print(advanced_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yKmfL_Uh1Tjj"
      },
      "id": "yKmfL_Uh1Tjj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}