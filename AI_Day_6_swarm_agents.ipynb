{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
        "outputId": "9b22a4a8-8577-4734-9d09-1d924a20c223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/swarm.git\n",
            "  Cloning https://github.com/openai/swarm.git to /tmp/pip-req-build-we93a6jb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/swarm.git /tmp/pip-req-build-we93a6jb\n",
            "  Resolved https://github.com/openai/swarm.git to commit 9db581cecaacea0d46a933d6453c312b034dbf47\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.33.0 in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.54.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (8.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (4.66.6)\n",
            "Collecting pre-commit (from swarm==0.1.0)\n",
            "  Downloading pre_commit-4.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting instructor (from swarm==0.1.0)\n",
            "  Downloading instructor-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.11.2)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.1.4)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.33.0->swarm==0.1.0)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (2.23.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2024.8.30)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading identify-2.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->swarm==0.1.0) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading virtualenv-20.28.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->swarm==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (1.5.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (0.1.2)\n",
            "Downloading instructor-1.7.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.0.1-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading identify-2.6.3-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: swarm\n",
            "  Building wheel for swarm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swarm: filename=swarm-0.1.0-py3-none-any.whl size=25999 sha256=965de2633a9d3822de68556947bc724f5cb28ffff2b9e42ca8f6f0df38486d32\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ad__imwv/wheels/46/9a/f7/7b8bbb674ae80ef0f62a632706c2c4cdfcf708e4da32e4e256\n",
            "Successfully built swarm\n",
            "Installing collected packages: distlib, virtualenv, nodeenv, jiter, identify, cfgv, pre-commit, instructor, swarm\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.7.1\n",
            "    Uninstalling jiter-0.7.1:\n",
            "      Successfully uninstalled jiter-0.7.1\n",
            "Successfully installed cfgv-3.4.0 distlib-0.3.9 identify-2.6.3 instructor-1.7.0 jiter-0.6.1 nodeenv-1.9.1 pre-commit-4.0.1 swarm-0.1.0 virtualenv-20.28.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting firecrawl-py\n",
            "  Downloading firecrawl_py-1.6.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl-py)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting websockets (from firecrawl-py)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2024.8.30)\n",
            "Downloading firecrawl_py-1.6.2-py3-none-any.whl (16 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, python-dotenv, firecrawl-py\n",
            "Successfully installed firecrawl-py-1.6.2 python-dotenv-1.0.1 websockets-14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/swarm.git\n",
        "!pip install openai\n",
        "!pip install firecrawl-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
        "outputId": "393a76c4-58a8-44c8-f2c2-b93fdbc9db5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"schema\" in \"FirecrawlApp.ExtractParams\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "from swarm import Agent, Swarm\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from openai import ChatCompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
        "outputId": "3119fa59-08c5-466f-8136-e906ad57e87c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Web Analytics Results:\n",
            "Here's the statistical summary and analysis of your dataset based on the provided data:\n",
            "\n",
            "### Overview\n",
            "\n",
            "#### Date\n",
            "- **Count**: 52,721\n",
            "- **Mean**: 2020-05-19 14:22:09\n",
            "- **Min**: 2020-01-01\n",
            "- **Max**: 2020-09-30\n",
            "\n",
            "#### Source\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 22\n",
            "- **Top Source**: Facebook (12,954 occurrences)\n",
            "\n",
            "#### Medium\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 6\n",
            "- **Top Medium**: CPC (16,833 occurrences)\n",
            "\n",
            "#### Delivery Available\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 3\n",
            "- **Top Value**: No data (31,953 occurrences)\n",
            "\n",
            "#### Device Type\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 3\n",
            "- **Top Device**: Mobile (24,416 occurrences)\n",
            "\n",
            "#### Promo Activated\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 2\n",
            "- **Top Value**: No (27,548 occurrences)\n",
            "\n",
            "#### Filter Used\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 2\n",
            "- **Top Value**: No (37,290 occurrences)\n",
            "\n",
            "### Page Interaction Metrics\n",
            "- **Pageviews**: \n",
            "  - **Mean**: 583.76\n",
            "  - **Min**: 0\n",
            "  - **Max**: 34,832\n",
            "  - **Std Dev**: 1,452.00\n",
            "\n",
            "- **Visits**: \n",
            "  - **Mean**: 127.12\n",
            "  - **Min**: 1\n",
            "  - **Max**: 6,975\n",
            "  - **Std Dev**: 384.87\n",
            "\n",
            "- **Product Clicks**: \n",
            "  - **Mean**: 890.76\n",
            "  - **Min**: 0\n",
            "  - **Max**: 32,460\n",
            "  - **Std Dev**: 2,100.56\n",
            "\n",
            "- **Add to Cart**: \n",
            "  - **Mean**: 177.97\n",
            "  - **Min**: 0\n",
            "  - **Max**: 6,486\n",
            "  - **Std Dev**: 419.90\n",
            "\n",
            "- **Checkout**: \n",
            "  - **Mean**: 187.59\n",
            "  - **Min**: 0\n",
            "  - **Max**: 4,936.96\n",
            "  - **Std Dev**: 441.13\n",
            "\n",
            "- **Transactions**: \n",
            "  - **Mean**: 20.09\n",
            "  - **Min**: 0\n",
            "  - **Max**: 1,113\n",
            "  - **Std Dev**: 52.43\n",
            "\n",
            "### Revenue\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 21,242\n",
            "- **Top Revenue**: ₱0 (14,292 occurrences)\n",
            "\n",
            "### Ad Spend\n",
            "- **Count**: 52,721\n",
            "- **Unique**: 51,201\n",
            "- **Top Spend**: ₱869,459 (3 occurrences)\n",
            "\n",
            "### Null Values\n",
            "- All fields in the dataset have zero null values, indicating a complete dataset.\n",
            "\n",
            "### Correlations\n",
            "- **Strongest Correlations**:\n",
            "  - Pageviews and Product Clicks (0.857)\n",
            "  - Product Clicks and Add to Cart (0.999)\n",
            "  - Pageviews and Visits (0.844)\n",
            "\n",
            "### Conclusion\n",
            "The dataset reflects a significant amount of page interaction with good coverage across various dimensions. The device type shows a strong mobile presence, and there are patterns in user engagements across different metrics, suggesting areas for focus in user experience strategies.\n",
            "\n",
            "If you need further analysis or specific insights, please let me know!\n",
            "\n",
            "Web Insights Results:\n",
            "Based on your dataset, here are the insights and actionable recommendations:\n",
            "\n",
            "### Insights\n",
            "\n",
            "1. **Traffic Sources**:\n",
            "   - The top traffic sources by pageviews are Google (12,296,272), Facebook (11,915,408) and TikTok (2,072,820). \n",
            "   - Direct visits are significant, with 2,929,713 pageviews.\n",
            "   - Other platforms like Cityads, Instagram, and Actionpay also contribute notably, but their volume is significantly lower compared to Google and Facebook.\n",
            "\n",
            "2. **Conversions**:\n",
            "   - Google is also the leading source for transactions (388,169), followed by Facebook (377,909) and TikTok (55,397).\n",
            "   - The transaction rates seem high on platforms with higher traffic volumes like Facebook and Google.\n",
            "\n",
            "3. **Engagement Metrics**:\n",
            "   - The number of product clicks highlights that Facebook generated the highest clicks (19,905,370), followed closely by Google (19,211,270).\n",
            "   - Direct product clicks show a significant figure at 3,886,140.\n",
            "\n",
            "4. **Underperforming Sources**:\n",
            "   - Some platforms like YouTube and Baidu show zero in transactions, indicating either poor targeting or a need for engagement strategies.\n",
            "\n",
            "5. **Newsletter Performance**:\n",
            "   - The newsletter contributes a fair number of visits (16,911) and transactions (1,955), but there's potential for growth.\n",
            "\n",
            "### Recommendations\n",
            "\n",
            "1. **Focus on High-Performing Sources**:\n",
            "   - Prioritize advertising on Google and Facebook where traffic and conversions are highest to maximize returns.\n",
            "\n",
            "2. **Optimize TikTok Marketing**:\n",
            "   - Given the high traffic from TikTok, consider optimizing content and ads specifically designed for this platform to capitalize on its potential.\n",
            "\n",
            "3. **Explore Underperforming Channels**:\n",
            "   - Investigate why channels like YouTube and Baidu are underperforming. If applicable, reallocate resources or discontinue efforts on these platforms.\n",
            "\n",
            "4. **Enhance Newsletter Strategy**:\n",
            "   - Strengthening the newsletter strategy can potentially increase engagement and conversion rates. Consider regular promotions or exclusive content.\n",
            "\n",
            "5. **Direct Engagement**:\n",
            "   - As direct traffic shows significant performance, ensure that systems and follow-ups are in place to capitalize on brand loyalty and returning visitors.\n",
            "\n",
            "6. **Content Diversification**:\n",
            "   - Utilize various content types (blogs, videos) on Facebook and Google to maintain fresh engagement, as they are the main sources of traffic and transactions.\n",
            "\n",
            "Overall, leveraging high-performing traffic sources while refining strategies on less effective channels will optimize your business outcomes.\n",
            "\n",
            "Web Analytics Results:\n",
            "I can definitely guide you on how to create visualizations using Seaborn or Matplotlib. However, I don't have the ability to access specific datasets directly, so I can provide examples based on a hypothetical dataset instead. Could you please describe the dataset or provide me with details about what data it contains? For example, what are the columns, and what specific insights or visuals are you interested in?\n"
          ]
        }
      ],
      "source": [
        "# Configure API Keys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]= \"\"\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jaydiaz2012/AI_First_Day_6_AI_Swarm/refs/heads/main/ai%20first%20sales%20data%20-%20sales%20(1).csv\")\n",
        "\n",
        "# Analyze dataset\n",
        "def analyze_dataset():\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    numerical_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "    summary = {\n",
        "        \"overview\": df.describe(include=\"all\").to_dict(),\n",
        "        \"null_values\": df.isnull().sum().to_dict(),\n",
        "        \"correlations\": numerical_df.corr().to_dict(),\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "def generate_web_insights():\n",
        "    web_insights = {\n",
        "        'source_pageviews': df.groupby('source')['pageviews'].sum(),\n",
        "        'source_visits': df.groupby('source')['visits'].sum(),\n",
        "        'source_transactions': df.groupby('source')['transactions'].sum(),\n",
        "        'source_product_click': df.groupby('source')['productClick'].sum()\n",
        "    }\n",
        "    return web_insights\n",
        "\n",
        "def generate_web_visuals():\n",
        "    visualizations = {}\n",
        "\n",
        "    # Page Views Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"pageviews\")\n",
        "    plt.title(\"Page Views Distribution\")\n",
        "    visualizations['page_views_distribution']\n",
        "    plt.show()\n",
        "\n",
        "    # Visits Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"visits\")\n",
        "    plt.title(\"Visits Distribution\")\n",
        "    visualizations['visits_distribution']\n",
        "    plt.show()\n",
        "\n",
        "    # Transactions Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"transactions\")\n",
        "    plt.title(\"Transactions Trends\")\n",
        "    visualizations['transactions_trends']\n",
        "    plt.show()\n",
        "\n",
        "    # Product Click Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"productClick\")\n",
        "    plt.title(\"Product Clicks Trends\")\n",
        "    visualizations['product_click_trends']\n",
        "    plt.show()\n",
        "\n",
        "    return visualizations\n",
        "\n",
        "web_analytics_agent = Agent(\n",
        "    name=\"Web Analyst Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a web analyst agent that cleans the dataset and report on the statistical summary of the dataset\"\"\",\n",
        "    functions=[analyze_dataset],\n",
        ")\n",
        "\n",
        "web_insights_agent = Agent(\n",
        "    name=\"Web Insights Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a Web Insights Agent that provide insights about the data provided and summarize the insights from other agents, including actionable recommendations\"\"\",\n",
        "    functions=[generate_web_insights]\n",
        ")\n",
        "\n",
        "web_insights_visuals_agent = Agent(\n",
        "    name=\"Web_Visuals_Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a data visualization expert that create visualizations on a given datase and gives summary of the data visuals\"\"\",\n",
        "    fuctions=[generate_web_visuals]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    client = Swarm()\n",
        "\n",
        "    web_analytics_response = client.run(\n",
        "        agent=web_analytics_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please clean and analyze my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Results:\")\n",
        "    print(web_analytics_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_response = client.run(\n",
        "        agent=web_insights_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide insights based from my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Insights Results:\")\n",
        "    print(web_insights_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_visuals_response = client.run(\n",
        "        agent=web_insights_visuals_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide charts using Seaborn or Matplotlib using the data from the given dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Results:\")\n",
        "    print(web_insights_visuals_response.messages[-1][\"content\"])\n",
        "\n",
        "# Advanced analysis using Swarm\n",
        "def advanced_analysis(prompt, model):\n",
        "    try:\n",
        "        prompt = (\n",
        "            \"Perform clustering, detect anomalies, and provide predictive trends \"\n",
        "            f\"on this dataset:\\n{df.to_csv(index=False)}\"\n",
        "        )\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a data analysis assistant using Swarm.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message['content']\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error performing advanced analysis: {e}\")\n",
        "        return None\n",
        "\n",
        "# Generate business insights\n",
        "def generate_insights(summary, model):\n",
        "    prompt = (\n",
        "        f\"Here is the dataset summary:\\n{summary}\\n\"\n",
        "        \"Generate actionable business insights from the above dataset.\"\n",
        "    )\n",
        "    response = model.create(\n",
        "        engine=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a data analyst AI.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "# Generate a report\n",
        "def generate_report(advanced_analysis, generate_insights, generate_web_visuals, file_path=\"report.txt\"):\n",
        "    with open(file_path, \"w\") as file:\n",
        "        file.write(\"Automated Business Insights Report\\n\")\n",
        "        file.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        file.write(\"**Insights:**\\n\")\n",
        "        file.write(insights + \"\\n\\n\")\n",
        "        file.write(\"**Advanced Analysis:**\\n\")\n",
        "        file.write(advanced_analysis_result + \"\\n\")\n",
        "    return file_path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yKmfL_Uh1Tjj"
      },
      "id": "yKmfL_Uh1Tjj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}