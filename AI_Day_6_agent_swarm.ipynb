{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
        "outputId": "9b22a4a8-8577-4734-9d09-1d924a20c223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/swarm.git\n",
            "  Cloning https://github.com/openai/swarm.git to /tmp/pip-req-build-we93a6jb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/swarm.git /tmp/pip-req-build-we93a6jb\n",
            "  Resolved https://github.com/openai/swarm.git to commit 9db581cecaacea0d46a933d6453c312b034dbf47\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.33.0 in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.54.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (8.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (4.66.6)\n",
            "Collecting pre-commit (from swarm==0.1.0)\n",
            "  Downloading pre_commit-4.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting instructor (from swarm==0.1.0)\n",
            "  Downloading instructor-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.11.2)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.1.4)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.33.0->swarm==0.1.0)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (2.23.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2024.8.30)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading identify-2.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->swarm==0.1.0) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading virtualenv-20.28.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->swarm==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (1.5.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (0.1.2)\n",
            "Downloading instructor-1.7.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.0.1-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading identify-2.6.3-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: swarm\n",
            "  Building wheel for swarm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swarm: filename=swarm-0.1.0-py3-none-any.whl size=25999 sha256=965de2633a9d3822de68556947bc724f5cb28ffff2b9e42ca8f6f0df38486d32\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ad__imwv/wheels/46/9a/f7/7b8bbb674ae80ef0f62a632706c2c4cdfcf708e4da32e4e256\n",
            "Successfully built swarm\n",
            "Installing collected packages: distlib, virtualenv, nodeenv, jiter, identify, cfgv, pre-commit, instructor, swarm\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.7.1\n",
            "    Uninstalling jiter-0.7.1:\n",
            "      Successfully uninstalled jiter-0.7.1\n",
            "Successfully installed cfgv-3.4.0 distlib-0.3.9 identify-2.6.3 instructor-1.7.0 jiter-0.6.1 nodeenv-1.9.1 pre-commit-4.0.1 swarm-0.1.0 virtualenv-20.28.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting firecrawl-py\n",
            "  Downloading firecrawl_py-1.6.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl-py)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting websockets (from firecrawl-py)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2024.8.30)\n",
            "Downloading firecrawl_py-1.6.2-py3-none-any.whl (16 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, python-dotenv, firecrawl-py\n",
            "Successfully installed firecrawl-py-1.6.2 python-dotenv-1.0.1 websockets-14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/swarm.git\n",
        "!pip install openai\n",
        "!pip install firecrawl-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
        "outputId": "393a76c4-58a8-44c8-f2c2-b93fdbc9db5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"schema\" in \"FirecrawlApp.ExtractParams\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "from swarm import Agent, Swarm\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from openai import ChatCompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
        "outputId": "ee6ec704-40c8-46fa-d278-e0518b82cef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Web Analytics Results:\n",
            "The dataset has been analyzed, and here are the results:\n",
            "\n",
            "### Overview of Dataset\n",
            "- **Total Records**: 52,721\n",
            "\n",
            "#### Date Information\n",
            "- **Mean Date**: May 19, 2020\n",
            "- **Min Date**: January 1, 2020\n",
            "- **Max Date**: September 30, 2020\n",
            "\n",
            "#### Categorical Variables:\n",
            "1. **Source**\n",
            "   - Unique Sources: 22\n",
            "   - Most Frequent Source: Facebook (12,954 occurrences)\n",
            "\n",
            "2. **Medium**\n",
            "   - Unique Mediums: 6\n",
            "   - Most Frequent Medium: CPC (16,833 occurrences)\n",
            "\n",
            "3. **Delivery Available**\n",
            "   - Unique Values: 3\n",
            "   - Most Frequent Value: No Data (31,953 occurrences)\n",
            "\n",
            "4. **Device Type**\n",
            "   - Unique Device Types: 3\n",
            "   - Most Frequent Device Type: Mobile (24,416 occurrences)\n",
            "\n",
            "5. **Promo Activated**\n",
            "   - Unique Values: 2\n",
            "   - Most Frequent Value: No (27,548 occurrences)\n",
            "\n",
            "6. **Filter Used**\n",
            "   - Unique Values: 2\n",
            "   - Most Frequent Value: No (37,290 occurrences)\n",
            "\n",
            "#### Numerical Variables:\n",
            "- **Pageviews**:\n",
            "   - Mean: 583.76\n",
            "   - Min: 0\n",
            "   - Max: 34,832\n",
            "   - Std Dev: 1,452.00\n",
            "\n",
            "- **Visits**:\n",
            "   - Mean: 127.12\n",
            "   - Min: 1\n",
            "   - Max: 6,975\n",
            "   - Std Dev: 384.87\n",
            "\n",
            "- **Product Clicks**:\n",
            "   - Mean: 890.76\n",
            "   - Min: 0\n",
            "   - Max: 32,460\n",
            "   - Std Dev: 2,100.56\n",
            "\n",
            "- **Add to Cart**:\n",
            "   - Mean: 177.97\n",
            "   - Min: 0\n",
            "   - Max: 6,486\n",
            "   - Std Dev: 419.90\n",
            "\n",
            "- **Checkout**:\n",
            "   - Mean: 187.59\n",
            "   - Min: 0\n",
            "   - Max: 4,936.96\n",
            "   - Std Dev: 441.13\n",
            "\n",
            "- **Transactions**:\n",
            "   - Mean: 20.09\n",
            "   - Min: 0\n",
            "   - Max: 1,113\n",
            "   - Std Dev: 52.43\n",
            "\n",
            "- **Revenue**:\n",
            "   - Total Unique Revenue Values: 21,242\n",
            "   - Most Frequent Revenue Value: 0 (14,292 occurrences)\n",
            "\n",
            "- **Ad Spend**:\n",
            "   - Total Unique Ad Spend Values: 51,201\n",
            "   - Top Ad Spend Value: 869,459 (3 occurrences)\n",
            "\n",
            "### Null Values\n",
            "- No null values in any of the fields.\n",
            "\n",
            "### Correlations\n",
            "- The following are some correlation coefficients among the numerical variables:\n",
            "  - **Pageviews** & **Visits**: 0.84\n",
            "  - **Product Clicks** & **Pageviews**: 0.86\n",
            "  - **Add to Cart** & **Product Clicks**: 0.999\n",
            "  - **Checkout** & **Transactions**: 0.68\n",
            "\n",
            "This shows strong positive correlations between interactions such as pageviews, product clicks, adds to cart, and transactions.\n",
            "\n",
            "### Summary\n",
            "- The dataset contains various metrics and interactions related to user engagement and conversions, indexed over time with no missing values. Insights into the source, medium, device type, and user actions (like clicking and converting) are available for further analysis.\n",
            "\n",
            "If you need specific visual representations or further analysis, feel free to ask!\n",
            "\n",
            "Web Insights Results:\n",
            "Based on the data provided, here are the insights and actionable recommendations:\n",
            "\n",
            "### Insights\n",
            "\n",
            "1. **Source Performance by Pageviews and Visits**:\n",
            "   - **Google** (12,296,272 pageviews) and **Facebook** (11,915,408 pageviews) are the top sources driving traffic, contributing significantly to the overall pageviews.\n",
            "   - **Direct traffic** accounts for a substantial amount of visits (883,431), indicating strong brand recognition or loyalty.\n",
            "   - **Tiktok** also shows a significant presence with 2,072,820 pageviews and 475,415 visits, showcasing its growing influence as a traffic source.\n",
            "\n",
            "2. **Transaction and Revenue Analysis**:\n",
            "   - Revenue from **Google** appears to lead with significant figures, while direct channels show a mix of transaction values that need further assessment for average value calculations.\n",
            "   - The data for revenue appears to contain inconsistencies or unstructured formats that should be cleaned for better analysis.\n",
            "\n",
            "3. **Product Clicks and Ad Spend**:\n",
            "   - High product clicks are noted across **Facebook** and **Google**, which suggests effective ad placements or content engagement on these platforms.\n",
            "   - It will be important to analyze return on ad spend (ROAS) on different platforms, especially comparing **Facebook** and **Google**, where user engagement is notably high.\n",
            "\n",
            "### Recommendations\n",
            "\n",
            "1. **Focus on High-Performing Sources**:\n",
            "   - Invest more resources into Google and Facebook for advertising due to their high traffic and engagement levels. Consider advanced targeting strategies to maximize ROI.\n",
            "\n",
            "2. **Explore Tiktok Marketing**:\n",
            "   - Since Tiktok has shown a substantial amount of pageviews and visits, consider an integrated marketing strategy focusing on this platform to further capitalize on its emerging audience.\n",
            "\n",
            "3. **Monitor Direct Traffic**:\n",
            "   - Investigate the factors contributing to extensive direct traffic. This could involve assessing branding efforts and customer loyalty programs.\n",
            "\n",
            "4. **Data Cleaning and Structuring**:\n",
            "   - The revenue figures appear unstructured. Implement data cleansing practices to standardize financial data collection. This ensures that revenue insights can be accurately analyzed for forecasting and strategic planning.\n",
            "\n",
            "5. **Implement A/B Testing**:\n",
            "   - Testing different content variations on high-traffic sources can yield insights into user preferences, helping optimize future campaigns across platforms.\n",
            "\n",
            "6. **Evaluate Ad Spend Efficiency**:\n",
            "   - Calculate the return on ad spend (ROAS) across various channels to determine the most cost-efficient platform for acquiring customers. Adjust budgets accordingly to favor high-ROAS channels.\n",
            "\n",
            "### Conclusion\n",
            "The data indicates strong performance from established platforms such as Google and Facebook, while Tiktok presents an opportunity for growth. Combining targeted marketing strategies with structured data analysis will enhance user engagement and increase revenue generation.\n",
            "\n",
            "Web Analytics Visuals Results:\n",
            "I am currently unable to create or display charts directly. However, I can guide you on how to create charts using the data from your dataset. If you provide me with the specific data or describe the dataset, I can suggest the appropriate types of charts to visualize the information effectively and summarize what insights could be gleaned from those visualizations. Please share more details about the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Configure API Keys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]= \"\"\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jaydiaz2012/AI_First_Day_6_AI_Swarm/refs/heads/main/ai%20first%20sales%20data%20-%20sales_final(web).csv\")\n",
        "\n",
        "# Analyze dataset\n",
        "def analyze_dataset():\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    numerical_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "    summary = {\n",
        "        \"overview\": df.describe(include=\"all\").to_dict(),\n",
        "        \"null_values\": df.isnull().sum().to_dict(),\n",
        "        \"correlations\": numerical_df.corr().to_dict(),\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "def generate_web_insights():\n",
        "    web_insights = {\n",
        "        'source_pageviews': df.groupby('source')['pageviews'].sum(),\n",
        "        'source_visits': df.groupby('source')['visits'].sum(),\n",
        "        'source_transactions': df.groupby('source')['revenue'].sum(),\n",
        "        'source_product_click': df.groupby('source')['ad spend'].sum()\n",
        "    }\n",
        "    return web_insights\n",
        "\n",
        "def generate_web_visuals():\n",
        "    visualizations = {}\n",
        "\n",
        "    # Page Views Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"pageviews\")\n",
        "    plt.title(\"Page Views Distribution\")\n",
        "    visualizations['page_views_distribution'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    # Visits Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"visits\")\n",
        "    plt.title(\"Visits Distribution\")\n",
        "    visualizations['visits_distribution'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    # Transactions Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"transactions\")\n",
        "    plt.title(\"Transactions Trends\")\n",
        "    visualizations['transactions_trends'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    # Product Click Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"productClick\")\n",
        "    plt.title(\"Product Clicks Trends\")\n",
        "    visualizations['product_click_trends'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    return visualizations\n",
        "\n",
        "web_analytics_agent = Agent(\n",
        "    name=\"Web Analyst Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a web analyst agent that cleans the dataset and report on the statistical summary of the dataset.\"\"\",\n",
        "    functions=[analyze_dataset],\n",
        ")\n",
        "\n",
        "web_insights_agent = Agent(\n",
        "    name=\"Web Insights Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a Web Insights Agent that provide insights about the data provided and summarize the insights from other agents, including actionable recommendations.\"\"\",\n",
        "    functions=[generate_web_insights]\n",
        ")\n",
        "\n",
        "web_insights_visuals_agent = Agent(\n",
        "    name=\"Web_Visuals_Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a data visualization expert that create visualizations on a given datase and gives summary of the data visuals.\"\"\",\n",
        "    fuctions=[generate_web_visuals]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    client = Swarm()\n",
        "\n",
        "    web_analytics_response = client.run(\n",
        "        agent=web_analytics_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please clean and analyze my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Results:\")\n",
        "    print(web_analytics_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_response = client.run(\n",
        "        agent=web_insights_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide insights based from my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Insights Results:\")\n",
        "    print(web_insights_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_visuals_response = client.run(\n",
        "        agent=web_insights_visuals_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide charts using the data from the given dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Visuals Results:\")\n",
        "    print(web_insights_visuals_response.messages[-1][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kguUQSmTIKBA"
      },
      "id": "kguUQSmTIKBA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}