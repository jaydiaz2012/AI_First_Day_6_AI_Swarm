{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12b1ec5e-0103-4407-afda-714bca5cb27c",
        "outputId": "9b22a4a8-8577-4734-9d09-1d924a20c223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/swarm.git\n",
            "  Cloning https://github.com/openai/swarm.git to /tmp/pip-req-build-we93a6jb\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/swarm.git /tmp/pip-req-build-we93a6jb\n",
            "  Resolved https://github.com/openai/swarm.git to commit 9db581cecaacea0d46a933d6453c312b034dbf47\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: openai>=1.33.0 in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (1.54.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (8.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from swarm==0.1.0) (4.66.6)\n",
            "Collecting pre-commit (from swarm==0.1.0)\n",
            "  Downloading pre_commit-4.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting instructor (from swarm==0.1.0)\n",
            "  Downloading instructor-1.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.33.0->swarm==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.11.2)\n",
            "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.16)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (3.1.4)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.33.0->swarm==0.1.0)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (2.23.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10.0.0,>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from instructor->swarm==0.1.0) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->swarm==0.1.0) (2024.8.30)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading identify-2.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting nodeenv>=0.11.1 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from pre-commit->swarm==0.1.0) (6.0.2)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit->swarm==0.1.0)\n",
            "  Downloading virtualenv-20.28.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (24.2)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1 in /usr/local/lib/python3.10/dist-packages (from pytest->swarm==0.1.0) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.1->instructor->swarm==0.1.0) (4.0.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.33.0->swarm==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.4->instructor->swarm==0.1.0) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai>=1.33.0->swarm==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.9.0->instructor->swarm==0.1.0) (1.5.4)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.10/dist-packages (from virtualenv>=20.10.0->pre-commit->swarm==0.1.0) (4.3.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->swarm==0.1.0) (0.1.2)\n",
            "Downloading instructor-1.7.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pre_commit-4.0.1-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Downloading identify-2.6.3-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading virtualenv-20.28.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: swarm\n",
            "  Building wheel for swarm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swarm: filename=swarm-0.1.0-py3-none-any.whl size=25999 sha256=965de2633a9d3822de68556947bc724f5cb28ffff2b9e42ca8f6f0df38486d32\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ad__imwv/wheels/46/9a/f7/7b8bbb674ae80ef0f62a632706c2c4cdfcf708e4da32e4e256\n",
            "Successfully built swarm\n",
            "Installing collected packages: distlib, virtualenv, nodeenv, jiter, identify, cfgv, pre-commit, instructor, swarm\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.7.1\n",
            "    Uninstalling jiter-0.7.1:\n",
            "      Successfully uninstalled jiter-0.7.1\n",
            "Successfully installed cfgv-3.4.0 distlib-0.3.9 identify-2.6.3 instructor-1.7.0 jiter-0.6.1 nodeenv-1.9.1 pre-commit-4.0.1 swarm-0.1.0 virtualenv-20.28.0\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.54.4)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Collecting firecrawl-py\n",
            "  Downloading firecrawl_py-1.6.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (2.32.3)\n",
            "Collecting python-dotenv (from firecrawl-py)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting websockets (from firecrawl-py)\n",
            "  Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from firecrawl-py) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->firecrawl-py) (2024.8.30)\n",
            "Downloading firecrawl_py-1.6.2-py3-none-any.whl (16 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading websockets-14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, python-dotenv, firecrawl-py\n",
            "Successfully installed firecrawl-py-1.6.2 python-dotenv-1.0.1 websockets-14.1\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/swarm.git\n",
        "!pip install openai\n",
        "!pip install firecrawl-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7339e0f-c07c-426e-880f-732e4be723ec",
        "outputId": "393a76c4-58a8-44c8-f2c2-b93fdbc9db5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:172: UserWarning: Field name \"schema\" in \"FirecrawlApp.ExtractParams\" shadows an attribute in parent \"BaseModel\"\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from firecrawl import FirecrawlApp\n",
        "from swarm import Agent, Swarm\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from openai import ChatCompletion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a9b89c0-1952-4e40-a6b6-ae2fcce1bcf0",
        "outputId": "9ed9721f-0f8e-46b6-eb80-384eac009ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Web Analytics Results:\n",
            "Here is the statistical summary and analysis of your dataset:\n",
            "\n",
            "### Overview of the Dataset\n",
            "\n",
            "- **Total Records**: 52,721\n",
            "\n",
            "#### Date\n",
            "- **Mean**: May 19, 2020\n",
            "- **Min**: January 1, 2020\n",
            "- **Max**: September 30, 2020\n",
            "\n",
            "#### Sources\n",
            "- **Unique**: 22\n",
            "- **Most Frequent Source**: Facebook (12,954 occurrences)\n",
            "\n",
            "#### Medium\n",
            "- **Unique**: 6\n",
            "- **Most Frequent Medium**: CPC (16,833 occurrences)\n",
            "\n",
            "#### Delivery Available\n",
            "- **Unique**: 3\n",
            "- **Most Frequent**: No Data (31,953 occurrences)\n",
            "\n",
            "#### Device Type\n",
            "- **Unique**: 3\n",
            "- **Most Frequent Device**: Mobile (24,416 occurrences)\n",
            "\n",
            "#### Promo Activated\n",
            "- **Unique**: 2\n",
            "- **Most Frequent**: No (27,548 occurrences)\n",
            "\n",
            "#### Filter Used\n",
            "- **Unique**: 2\n",
            "- **Most Frequent**: No (37,290 occurrences)\n",
            "\n",
            "### Numerical Metrics\n",
            "\n",
            "#### Pageviews\n",
            "- **Mean**: 583.76\n",
            "- **Min**: 0\n",
            "- **Max**: 34,832\n",
            "- **Standard Deviation**: 1,452.00\n",
            "\n",
            "#### Visits\n",
            "- **Mean**: 127.12\n",
            "- **Min**: 1\n",
            "- **Max**: 6,975\n",
            "- **Standard Deviation**: 384.87\n",
            "\n",
            "#### Product Clicks\n",
            "- **Mean**: 890.76\n",
            "- **Min**: 0\n",
            "- **Max**: 32,460\n",
            "- **Standard Deviation**: 2,100.56\n",
            "\n",
            "#### Add to Cart\n",
            "- **Mean**: 177.97\n",
            "- **Min**: 0\n",
            "- **Max**: 6,486\n",
            "- **Standard Deviation**: 419.90\n",
            "\n",
            "#### Checkout\n",
            "- **Mean**: 187.59\n",
            "- **Min**: 0\n",
            "- **Max**: 4,936.96\n",
            "- **Standard Deviation**: 441.13\n",
            "\n",
            "#### Transactions\n",
            "- **Mean**: 20.09\n",
            "- **Min**: 0\n",
            "- **Max**: 1,113\n",
            "- **Standard Deviation**: 52.43\n",
            "\n",
            "#### Revenue\n",
            "- **Unique Entries**: 21,242\n",
            "- **Most Frequent**: 0 (14,292 occurrences)\n",
            "\n",
            "#### Ad Spend\n",
            "- **Unique Entries**: 51,201\n",
            "- **Most Frequent Entry**: 869,459 (3 occurrences)\n",
            "\n",
            "### Null Values\n",
            "- No null values detected in any columns.\n",
            "\n",
            "### Correlations\n",
            "- **Pageviews** correlate strongly with:\n",
            "  - Visits: 0.84\n",
            "  - Product Clicks: 0.86\n",
            "  - Add to Cart: 0.86\n",
            "  - Checkout: 0.83\n",
            "  - Transactions: 0.65\n",
            "\n",
            "- **Product Clicks** and **Add to Cart** show a near-perfect correlation of 0.99999, indicating linked behaviors.\n",
            "\n",
            "### Summary\n",
            "This dataset shows a well-maintained collection of web analytics data, with no missing values. The distribution of pageviews, visits, and other metrics indicates varying engagement with the website, with particularly strong activity observed from mobile devices. The correlations suggest that higher pageviews and visits lead to increased product clicks and add-to-cart actions, highlighting the importance of user interactions in driving conversions. \n",
            "\n",
            "If you need further specific analyses or visualizations, please let me know!\n",
            "\n",
            "Web Insights Results:\n",
            "### Summary of Insights\n",
            "\n",
            "Based on the provided dataset, here are the key insights drawn from various sources of traffic, page views, visits, transactions, and ad spend:\n",
            "\n",
            "1. **Traffic Sources**:\n",
            "   - **Google** and **Facebook** are the dominant traffic sources, with page views of **12,296,272** and **11,915,408**, respectively. This implies a strong performance in organic search and social media engagement.\n",
            "   - **Direct traffic** also shows a significant presence with **2,929,713** page views, indicating a solid base of returning users or brand recall.\n",
            "   - Lesser-known sources like **DuckDuckGo**, **Bing**, and **Baidu** contribute comparably small traffic figures, suggesting potential opportunities for SEO enhancements.\n",
            "\n",
            "2. **Visits**:\n",
            "   - Similar to page views, Google and Facebook lead in visits, with **2,497,758** and **2,464,741** visits respectively.\n",
            "   - The **Tiktok** channel has garnered **475,415** visits, indicating its growing importance in your marketing strategy.\n",
            "\n",
            "3. **Transactions**:\n",
            "   - Facebook stands out in generating revenue, with transaction figures suggesting possible strong conversion rates from its traffic. However, there is a lack of specific numeric values for total transactions in the dataset, making it hard to analyze thoroughly.\n",
            "   - Ad spend data shows significant investment in sources like Google and Instagram, yet the corresponding revenue from these platforms is hard to interpret without precise transaction numbers.\n",
            "\n",
            "4. **Ad Spend**:\n",
            "   - The ad spend is noted across different sources, with **Facebook** and **Google** receiving substantial portions. However, absolute revenue figures are presented in an unreadable format (e.g., \"000000000000000...\"), necessitating cleaning or restructuring for accurate analysis.\n",
            "   - Notable returns from promotional campaigns can be implied, yet detailed financial metrics are hindered by formatting issues.\n",
            "\n",
            "### Actionable Recommendations\n",
            "\n",
            "1. **Enhance Performance on Leading Channels**:\n",
            "   - Invest further in Google and Facebook advertising to build on existing user engagement. The high volume suggests a receptive audience.\n",
            "\n",
            "2. **Explore Expanding New Channels**:\n",
            "   - Given Tiktok's growth in visits, consider dedicating resources to improve brand presence or advertising efforts on this platform.\n",
            "\n",
            "3. **SEO Optimization for Lesser Sources**:\n",
            "   - Address SEO strategies to improve visibility and engagement on poorly performing search engines like DuckDuckGo and Baidu.\n",
            "\n",
            "4. **Focus on Conversion Metrics**:\n",
            "   - Improve tracking and insights on conversions from various traffic sources to pinpoint high-performing strategies and allocate budgets effectively. Fixing the formatting of revenue figures can assist with this.\n",
            "\n",
            "5. **Retargeting Campaigns**:\n",
            "   - Leverage insights on direct traffic; consider retargeting campaigns for users who have previously engaged with the brand to enhance conversion chances.\n",
            "\n",
            "6. **Data Quality Improvement**:\n",
            "   - Clean and structure the financial data for transactions and ad spend to enable comprehensive analyses. This will enhance decision-making and ROI evaluations from different channels.\n",
            "\n",
            "These strategies can help drive traffic, improve user engagement, and ultimately boost sales conversions across channels.\n",
            "\n",
            "Web Analytics Visuals Results:\n",
            "I can guide you on how to create visualizations using Seaborn or Matplotlib in Python, but I need you to provide the dataset or specific details about it. Please upload the dataset or describe its contents, and I'll help you with the corresponding code to generate charts.\n"
          ]
        }
      ],
      "source": [
        "# Configure API Keys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"]= \"\"\n",
        "\n",
        "#Load dataset\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/jaydiaz2012/AI_First_Day_6_AI_Swarm/refs/heads/main/ai%20first%20sales%20data%20-%20sales_final(web).csv\")\n",
        "\n",
        "# Analyze dataset\n",
        "def analyze_dataset():\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    numerical_df = df.select_dtypes(include=np.number)\n",
        "\n",
        "    summary = {\n",
        "        \"overview\": df.describe(include=\"all\").to_dict(),\n",
        "        \"null_values\": df.isnull().sum().to_dict(),\n",
        "        \"correlations\": numerical_df.corr().to_dict(),\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "def generate_web_insights():\n",
        "    web_insights = {\n",
        "        'source_pageviews': df.groupby('source')['pageviews'].sum(),\n",
        "        'source_visits': df.groupby('source')['visits'].sum(),\n",
        "        'source_transactions': df.groupby('source')['revenue'].sum(),\n",
        "        'source_product_click': df.groupby('source')['ad spend'].sum()\n",
        "    }\n",
        "    return web_insights\n",
        "\n",
        "def generate_web_visuals():\n",
        "    visualizations = {}\n",
        "\n",
        "    # Page Views Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"pageviews\")\n",
        "    plt.title(\"Page Views Distribution\")\n",
        "    visualizations['page_views_distribution'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    # Visits Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"visits\")\n",
        "    plt.title(\"Visits Distribution\")\n",
        "    visualizations['visits_distribution'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    # Transactions Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"transactions\")\n",
        "    plt.title(\"Transactions Trends\")\n",
        "    visualizations['transactions_trends'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    # Product Click Trends\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(df, x=\"source\", y=\"productClick\")\n",
        "    plt.title(\"Product Clicks Trends\")\n",
        "    visualizations['product_click_trends'] = plt.gcf()\n",
        "    plt.close()\n",
        "\n",
        "    return visualizations\n",
        "\n",
        "web_analytics_agent = Agent(\n",
        "    name=\"Web Analyst Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a web analyst agent that cleans the dataset and report on the statistical summary of the dataset\"\"\",\n",
        "    functions=[analyze_dataset],\n",
        ")\n",
        "\n",
        "web_insights_agent = Agent(\n",
        "    name=\"Web Insights Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a Web Insights Agent that provide insights about the data provided and summarize the insights from other agents, including actionable recommendations\"\"\",\n",
        "    functions=[generate_web_insights]\n",
        ")\n",
        "\n",
        "web_insights_visuals_agent = Agent(\n",
        "    name=\"Web_Visuals_Agent\",\n",
        "    model=\"gpt-4o-mini\",\n",
        "    instructions=\"\"\"You are a data visualization expert that create visualizations on a given datase and gives summary of the data visuals\"\"\",\n",
        "    fuctions=[generate_web_visuals]\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    client = Swarm()\n",
        "\n",
        "    web_analytics_response = client.run(\n",
        "        agent=web_analytics_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please clean and analyze my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Results:\")\n",
        "    print(web_analytics_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_response = client.run(\n",
        "        agent=web_insights_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide insights based from my dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Insights Results:\")\n",
        "    print(web_insights_response.messages[-1][\"content\"])\n",
        "\n",
        "    web_insights_visuals_response = client.run(\n",
        "        agent=web_insights_visuals_agent,\n",
        "        messages=[{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Please provide charts using Seaborn or Matplotlib using the data from the given dataset.\"\n",
        "        }]\n",
        "    )\n",
        "\n",
        "    print(\"\\nWeb Analytics Visuals Results:\")\n",
        "    print(web_insights_visuals_response.messages[-1][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kguUQSmTIKBA"
      },
      "id": "kguUQSmTIKBA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}